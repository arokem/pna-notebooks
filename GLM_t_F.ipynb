{
 "metadata": {
  "name": "GLM_t_F"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "An introduction to the General Linear Model, t and F tests"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "This notebook comprises two sections. The first section is an introduction to linear regression by Ariel, the second takes an example from fmri data and explain t and F statistics.\n",
      "\n",
      "It builds upon the following chapter : http://www.fil.ion.ucl.ac.uk/spm/doc/books/hbf2/pdfs/Ch8.pdf in the book Human Brain Function (2nd edition) and upon the review in Neuroimage (Poline and Brett, 2012).\n",
      "\n",
      "In this notebook, in a first part we will explain the General Linear Model in a simple case, assuming first that we are analyzing only one region of interest or one voxel.\n",
      "\n",
      "In a second part, we will take an example (cf the \"principle of stat\" notebook) and run an analysis at each and every voxel."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Linear Regression - introduction (by Ariel Rokem)\n",
      "\n",
      "This section is based in part on chapter 3 of \"The elements of statistical learning\" by Hastie, Tibshirani and Friedman (get full pdf [here](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)). \n",
      "\n",
      "## Why model? \n",
      "\n",
      "Models are mathematical constructs used to explain measured data. In cognitive neuroscience, models are usually used to quantitatively describe specific parts of the nervous system (e.g. neurons, voxels, etc.). Typically, we seek to understand changes in the level of a signal measured from the unit (e.g. spike rate, or BOLD response), as a function of the inputs into the system (e.g. stimulus presented, cognitive state of the participant, etc.). Accurate and precise mathematical models derived from the data are useful in the following ways: \n",
      "\n",
      "- **They allow you to explain the data you have**: summary statistics of the data, such as the mean of the responses of a voxel, or the variance of this variable, are often not that informative (especially if normalized...). A model can give you a good explanation of the data you have observed, in terms of background knowledge, or theory.\n",
      "\n",
      "- **They allow you to predict data you haven't collected**: In many cases, assuming that the parameters don't change with changes in the independent variables, models allow us to interpolate and predict what the dependent variables would have been like in other values of the independent variables. Even extrapolate to conditions that have not measured. For example, because measuring in these conditions is difficult for practical reasons. \n",
      "\n",
      "- **They allow you to specify your confidence in a specific theory**: By performing some form of statistical testing, you can specify your confidence level in the veracity of statements about responses to a particular condition, or differential responses in different conditions."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Definition of a linear model:\n",
      "\n",
      "Assume that we make a measurement $\\bf{y}$. Here *bold-face* indicates that we are talking about a measurment *vector*. Say, of length $N$. This measurement is a response in our unit-of-interest to some inputs: $(\\bf{x}_1, \\bf{x}_2, ... \\bf{x}_p)$. Each one of the $\\bf{x}_i$ vectors also has $N$ elements. We will make this more concrete later on, but for now, you can think of the $\\bf{X}$ as representing different input streams. For example, different locations on the screen.\n",
      "\n",
      "A linear model assumes that the expected value of $\\bf{y}$ given a specific input $\\bf{X}$ can be expressed as a linear sum of the $\\bf{x}_i$. That is: $E(\\bf{Y}|\\bf{X})$ is linear in the inputs. \n",
      "\n",
      "Let's break that down a bit. All this means is that in the noise-less case $\\bf{y}$ is a function of the inputs, $f(x)$, such that there exist $\\beta_i$ which satisfy: \n",
      "\n",
      "$f(x) = \\beta_0 + \\beta_1 \\bf{x}_1 + \\beta_2 x_2 + ... + \\beta_p x_p$ \n",
      "\n",
      "You will notice that bivariate linear regression, which you are probably familiar with is simply a subset of this, with N=1"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def linear_function(x,a,b):\n",
      "    \"\"\" \n",
      "    A linear function\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    x : ndarray\n",
      "        Input variable\n",
      "    \n",
      "    a : float\n",
      "        offset parameter\n",
      "    \n",
      "    b : float \n",
      "        slope parameter\n",
      "    \"\"\"\n",
      "    return a + b * x \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N = 100\n",
      "x = np.linspace(-pi, pi, N)\n",
      "\n",
      "a = np.random.randn() * 10\n",
      "b = np.random.randn() * 10\n",
      "y = linear_function(x, a, b)\n",
      "\n",
      "fig, ax = plt.subplots(1)\n",
      "ax.plot(x, y, '.')\n",
      "ax.set_xlabel('x')\n",
      "ax.set_ylabel('y')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t = np.linspace(-pi, pi, N)\n",
      "x = np.sin(t)\n",
      "\n",
      "fig, ax = plt.subplots(1)\n",
      "ax.plot(x)\n",
      "\n",
      "y = linear_function(x, a, b)\n",
      "fig, ax = plt.subplots(1)\n",
      "ax.plot(y, '.')\n",
      "\n",
      "fig, ax = plt.subplots(1)\n",
      "ax.plot(x, y, '.')\n",
      "ax.set_xlabel('x')\n",
      "ax.set_ylabel('y')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Multi-linear regression: \n",
      "\n",
      "Now, let's generalize this to $p$ inputs, where we choose $p=10$\n",
      "\n",
      "For example, let's consider an output which is a linear combination of $p$ sine waves. \n",
      "\n",
      "To account for $\\beta_0$, we make sure that the first column is all 1's:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "p = 10  # Number of inputs\n",
      "\n",
      "# Preallocate a matrix (make it a matrix of ones to account for beta_0):\n",
      "X = np.ones((N, p)) \n",
      "for ii in range(1, p):\n",
      "    X[:, ii] = np.sin((ii+1)*t)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(1) \n",
      "ax.plot(t,X)\n",
      "ax.set_ylim([-1.1, 1.1])\n",
      "ax.set_xlim([-pi, pi])\n",
      "ax.set_xlabel('t')\n",
      "ax.set_ylabel('x')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def linear_model(X, beta):\n",
      "    \"\"\" \n",
      "    The linear model\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    X : 2d array\n",
      "        The design matrix : a matrix of regressors\n",
      "    \n",
      "    beta : 1d array \n",
      "        Model coefficients\n",
      "    \"\"\" \n",
      "    return np.dot(X, beta)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Where `np.dot` is a matrix multiplication:\n",
      "\n",
      "$\\bf{X}\\beta = \\beta_0 + \\beta_1 \\bf{x}_1 + \\beta_2 \\bf{x}_2 + ... + \\beta_p \\bf{x}_p $\n",
      "\n",
      "## Aside on matrix multiplication : \n",
      "\n",
      "The multiplication of two matrices $\\bf{A}$ and $\\bf{B}$ is denoted $\\bf{AB}$. This is only defined if the *inner* dimension of the multiplication matches. That is, if $\\bf{A}$ is $m$ by $n$ and $\\bf{B}$ is $n$ by $k$. In that case, element $i,j$ of the multiplication is defined as following: \n",
      "\n",
      "$\\bf{AB}_{ij} = \\sum_l\\sum_k{A_{ik} B_{lj}}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Back to our regression problem\n",
      "\n",
      "We choose a set of random $\\beta$ coefficients and generate $f(x)$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "beta = np.random.randn(p)\n",
      "f = linear_model(X, beta)\n",
      "fig, ax = plt.subplots(1)\n",
      "ax.plot(t, f)\n",
      "ax.set_xlim([-pi, pi])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this case, $\\bf{y}$ is not going to be linear in any particular $\\bf{x}_i$, only in a combination of them:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(1)\n",
      "ax.plot(X[:, 1], y, '.')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Noise\n",
      "\n",
      "Unfortunately, the noise-less case is not a very realistic one. Let's assume that the output is additionaly corrupted by noise, $\\epsilon$: \n",
      "\n",
      "$\\bf{y} = f(\\bf{x}) + \\epsilon$\n",
      "\n",
      "which is distributed according to a Gaussian/normal distribution with zero mean:\n",
      "\n",
      "$\\epsilon \\sim \\mathcal{N}(0, \\sigma)$\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y = f + 3*np.random.randn(*f.shape)\n",
      "\n",
      "fig, ax = plt.subplots(1)\n",
      "ax.plot(f, label='f')\n",
      "ax.plot(y, label='y')\n",
      "plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Solving the linear model\n",
      "\n",
      "Solving the linear model, means finding an estimate of the $\\bf{\\beta}$ coefficients that generated the data we observe\n",
      "\n",
      "In other words, we are looking for procedure to estimate the coefficients (we call the estimated coefficients $\\bf{\\hat{\\beta}}$)\n",
      "\n",
      "\n",
      "### Criterion for a solution\n",
      "\n",
      "A 'good' estimate of these coefficients would produce an estimate of the output, $\\bf{\\hat{y}}$, which would be close to the measured data points.\n",
      "\n",
      "Where: \n",
      "\n",
      "$\\bf{\\hat{y}} = \\bf{ X\\hat{\\beta}}$\n",
      "\n",
      "One way to define this is by saying that we want the residual sum of squares to be small, where the residuals are: \n",
      "\n",
      "$\\bf{y}-\\bf{\\hat{y}}$ \n",
      "\n",
      "and the residual sum of squares (or RSS) is: \n",
      "\n",
      "$RSS =  \\sum_{i=1}^{N}(y_i - \\hat{y_i})^2$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Finding $\\beta$  - a first solution\n",
      "\n",
      "\n",
      "Another way of deriving the result is to minimize the sum of square of our residuals:\n",
      "$\\sum_i \\hat\\epsilon_i^2 = \\sum_i{(Y_i - \\widehat{Y}_i)^2} $ :\n",
      "\n",
      "\n",
      "Since $\\bf{\\hat{y}}$ is a function of $\\bf{\\beta}$, we can think of $RSS$ as a function of $\\beta$ and we want to optimize it with respect to a choice of the elements of $\\beta$. That is, we want to find a minimum of $RSS$ with respect to $\\beta$. \n",
      "\n",
      "Mathematically speaking, we want to find a value of $\\bf{\\hat{\\beta}}$ for which the first derivative of $RSS$ with respect to $\\beta$ is 0 \n",
      "\n",
      "To find the derivative, we re-write $RSS$ in the following way: \n",
      "\n",
      "$RSS(\\beta) = (\\bf{y} - X\\beta)^T (\\bf{y} - X\\beta) $\n",
      "The derivative of this function is (we leave it as an exercise to the reader to prove this:)\n",
      "\n",
      "$\\frac{\\partial RSS}{\\partial\\beta} = -2 \\bf{X}^T (\\bf{y} - \\bf{X}\\beta)$\n",
      "\n",
      "Setting the first derivative to 0 and dividing both sides by 2, we obtain\n",
      "\n",
      "$\\bf{X}^T (y-\\bf{X}\\beta) = 0$\n",
      "\n",
      "$\\bf{X}^T y - \\bf{X}^T \\bf{X} \\beta = 0$\n",
      "\n",
      "$\\Rightarrow \\bf{X}^T y = \\bf{X}^T \\bf{X} \\beta$ \n",
      "\n",
      "If $\\bf{X}$ is 'full rank', then $\\bf{X}^T\\bf{X}$ is [positive definite](http://en.wikipedia.org/wiki/Positive-definite_matrix) and it can be inverted. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.linalg as lin\n",
      "\n",
      "def ols(X):\n",
      "    \"\"\"\n",
      "    The matrix which solves the OLS regression problem for full-rank X\n",
      "    \"\"\"\n",
      "    return np.dot(lin.pinv(np.dot(X.T, X)), X.T)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "beta_hat = np.dot(ols(X), y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot(beta, beta_hat, 'o')\n",
      "plt.xlabel('Real parameter value')\n",
      "plt.ylabel('Estimate parameter value')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using $\\bf{\\hat{\\beta}}$, we can calculate back $\\bf{\\hat{y}}$, the estimate of the output and compare it to the actual output: \n",
      "\n",
      "$\\bf{\\hat{y}} = \\bf{X} \\bf{\\hat{\\beta}}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_hat = np.dot(X, beta_hat)\n",
      "fig, ax = plt.subplots(1)\n",
      "ax.plot(y, label='$y$')\n",
      "ax.plot(y_hat, label='$\\hat{y}$')\n",
      "plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The so called \"hat matrix\", or $ H $ is the matrix which defines the transformation from $\\bf{X}$ to the estimate $\\hat{y}$, so it's the matrix that \"puts the hat\" on $y$: \n",
      "\n",
      "$H = \\bf{X}(\\bf{X^T}\\bf{X})^{-1} \\bf{X^T}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "H = np.dot(X, ols(X))\n",
      "H.shape\n",
      "y_hat = np.dot(H.T, y)\n",
      "\n",
      "plot(y, y_hat, 'o')\n",
      "plt.xlabel('The original output') \n",
      "plt.ylabel('Linear estimate of the output')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "GLM, t and F-tests"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Grasping some data"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "First, finding some nifti or img on the disk. Let's assume we have some data in our current directory:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nibabel as nib\n",
      "import os\n",
      "from os.path import join as pjoin\n",
      "\n",
      "this_course = 'pna'\n",
      "\n",
      "def load_images(course):\n",
      "    if course == 'pna':\n",
      "        PNA_DATA_PATH = pjoin(os.path.expanduser('~'), 'data', 'qcpna', 'bolddata')\n",
      "        img_fname = pjoin(PNA_DATA_PATH, 'smallbold.nii.gz')\n",
      "        if os.path.isfile(img_fname):\n",
      "            img = nib.load(img_fname)\n",
      "            print img.shape\n",
      "            return img\n",
      "        else:\n",
      "            print \"where is this image: \" + fname + \"\\n\"\n",
      "            return None\n",
      "                     \n",
      "    else:\n",
      "        files_1 = [pjoin('ds107','sub' + \"%03d\" %i, 'BOLD','task001_run001','meanabold.nii') for i in range(1,15)]\n",
      "        print [(f,os.path.isfile(f)) for f in files_1]\n",
      "        #files = [os.path.join(scans_dir, 'snn03055dy%i.img' % i) for i in range(1,13)]\n",
      "        files = [f for f in files_1 if os.path.isfile(f)]\n",
      "        return [nib.load(f) for f in files]\n",
      "    \n",
      "img = load_images('pna')"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Once we have these images, we want in a first step to extract one voxel value. To do so, we know that images spatially normalized have an affine transform that allows us to compute the the position of a (i,j,k) index of the image in the template space. If we have the (x,y,z) position in the template space, we need to inverse this transform to get the index of the (closest voxel).  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Now we load the affine transformation from the first image (it's the same for all), pick a voxel in the MNI space, and compute the conversion parameters from MNI space (in mm) to voxels space (indices of the 3D array)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# transformation from voxel no to mm\n",
      "\n",
      "try:\n",
      "    M = img.get_affine()\n",
      "except: \n",
      "    M = img[0].get_affine()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# mm to voxel no\n",
      "iM = np.linalg.inv(M)\n",
      "# coordinates of voxel of interest in mm (MNI space)\n",
      "posmm = [-20.0, -42, 34]\n",
      "(x, y, z) = tuple(posmm)\n",
      "# coordinates in voxel space (in homogenous coordinates)\n",
      "posvox = np.dot(iM, posmm + [1])\n",
      "# We grab the spatial part of the output.  Since we want to use it as an \n",
      "# index, we need to make it a tuple\n",
      "i,j,k = tuple(np.round(posvox[:3]).astype(int))\n",
      "\n",
      "print i,j,k\n",
      "vdata = img.get_data()[i,j,k, :]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Let's display the values of this voxel - and display a slice of the 4D volume."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(14,4))\n",
      "ax1.plot(vdata); ax1.set_title('the time series')\n",
      "ax2.set_title('the slice at z = %d' %z)\n",
      "# time t = 0\n",
      "t = 0\n",
      "imshow(img.get_data()[:,:,z,t], interpolation='nearest')"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The GLM machinery: any difference with linear regression?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "We will model the data using the GLM, then ask questions on the parameters of this model, a concise and fairly general framework for asking question to our data. \n",
      "\n",
      "$$\n",
      "Y = X \\beta + \\epsilon\n",
      "$$ \n",
      "\n",
      "In a first example, we take X to contain two columns. Let's assume for a moment that our data come from an experiment with 12 subjects in which odd scans are from condition 1 (or group one if each scan is from a different subject) and even scans are from group 2. The first column of X will model this with, with $x_{i1} = 1$ when i is odd, and $x_{i1} = -1$ if $i$ even. The second column will be a column of ones, modelling a constant offset. We note $\\epsilon_i$ the noise at each measurement $y_i$ under this model.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "$$ Y = \\left[ \n",
      "  \\begin{array}{c} \n",
      "  y_1  \\\\\\\\ y_2  \\\\\\\\ y_3  \\\\\\\\ \n",
      "  y_4  \\\\\\\\ y_5  \\\\\\\\ y_6  \\\\\\\\ \n",
      "  y_7 \\\\\\\\ y_8  \\\\\\\\ y_9  \\\\\\\\ \n",
      "  y_{10} \\\\\\\\ y_{11}  \\\\\\\\ y_{12}  \\\\\\\\ \n",
      "  \\end{array} \n",
      "\\right] \n",
      "= \\beta_0  \n",
      "\\left[ \\begin{array}{c} \n",
      "  x_1  \\\\\\\\ x_2  \\\\\\\\ x_3  \\\\\\\\ \n",
      "  x_4  \\\\\\\\ x_5  \\\\\\\\ x_6  \\\\\\\\ \n",
      "  x_7  \\\\\\\\ x_8  \\\\\\\\ x_9  \\\\\\\\ \n",
      "  x_{10} \\\\\\\\ x_{11}  \\\\\\\\ x_{12}  \\\\\\\\ \n",
      "\\end{array} \\right] \n",
      "+\n",
      "\\beta_1  \n",
      "\\left[ \\begin{array}{c} \n",
      "%  \\small{\\textrm{1}}   \\\\\\\\  \\small{\\textrm{1}}  \\\\\\\\ \\small{\\textrm{1}}  \\\\\\\\  \\small{\\textrm{1}}  \\\\\\\\   \n",
      "%  \\small{\\textrm{1}}   \\\\\\\\  \\small{\\textrm{1}}  \\\\\\\\ \\small{\\textrm{1}}  \\\\\\\\  \\small{\\textrm{1}}  \\\\\\\\   \n",
      "%  \\small{\\textrm{1}}   \\\\\\\\  \\small{\\textrm{1}}  \\\\\\\\ \\small{\\textrm{1}}  \\\\\\\\  \\small{\\textrm{1}}  \\\\\\\\   \n",
      "   1 \\\\\\\\  1  \\\\\\\\  1  \\\\\\\\  1  \\\\\\\\   \n",
      "   1 \\\\\\\\  1  \\\\\\\\  1  \\\\\\\\  1  \\\\\\\\   \n",
      "   1 \\\\\\\\  1  \\\\\\\\  1  \\\\\\\\  1  \\\\\\\\   \n",
      "\\end{array} \\right] \n",
      "+ \n",
      "\\left[ \\begin{array}{c}\n",
      "\t\\epsilon_1 \\\\\\\\ \\epsilon_2 \\\\\\\\ \\epsilon_3 \\\\\\\\ \\epsilon_4 \\\\\\\\ \\epsilon_5 \\\\\\\\ \\epsilon_6 \\\\\\\\\n",
      "\t \\epsilon_7 \\\\\\\\ \\epsilon_8 \\\\\\\\ \\epsilon_9 \\\\\\\\ \\epsilon_{10} \\\\\\\\ \\epsilon_{11} \\\\\\\\ \\epsilon_{12} \\\\\\\\ \\end{array} \\right]\n",
      "= \\left[ \n",
      "\\begin{array}{rc} \n",
      "  1 & 1 \\\\\\\\ -1 & 1  \\\\\\\\ 1 & 1  \\\\\\\\ -1 & 1  \\\\\\\\   \n",
      "  1 & 1 \\\\\\\\ -1 & 1  \\\\\\\\ 1 & 1  \\\\\\\\ -1 & 1  \\\\\\\\   \n",
      "  1 & 1 \\\\\\\\ -1 & 1  \\\\\\\\ 1 & 1  \\\\\\\\ -1 & 1  \\\\\\\\   \n",
      "\\end{array} \\right] \n",
      "\\left[ \\begin{array}{c}\n",
      "\\beta_0 \\\\\\\\ \\beta_1 \\\\\\\\\n",
      "\\end{array} \\right] +\n",
      "\\left[ \\begin{array}{c}\n",
      "\t\\epsilon_1 \\\\\\\\ \\epsilon_2 \\\\\\\\ \\epsilon_3 \\\\\\\\ \\epsilon_4 \\\\\\\\ \\epsilon_5 \\\\\\\\ \\epsilon_6 \\\\\\\\\n",
      "\t \\epsilon_7 \\\\\\\\ \\epsilon_8 \\\\\\\\ \\epsilon_9 \\\\\\\\ \\epsilon_{10} \\\\\\\\ \\epsilon_{11} \\\\\\\\ \\epsilon_{12} \\\\\\\\ \\end{array} \\right]\n",
      "= X \\bf \\beta +  \\bf \\epsilon\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Finding $\\beta$ - a second solution\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "In the equation above, we know the model $X$ (or at least we assume this model), and we need to estimate the $\\beta$ and the $\\epsilon$. To **estimate** these, the simplest is to first multiply the left and right hand side of the equation by $X^T$, the transpose of $X$, to get: "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "$$ X^T Y = X^T X \\beta + X^T \\epsilon $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "In the above equation, we search for an estimate of the noise ($\\hat\\epsilon$) that is **uncorrelated** with our explanatory variables (the constant vector or ones, and the $x_{i1}$). In other words, in matrix terms, such that $X^T \\hat\\epsilon = 0$. In terms of the *estimates* $\\hat\\beta$ and $\\hat\\epsilon$, we therefore have:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "$$ X^T Y = X^T X \\hat\\beta + X^T \\hat\\epsilon = X^T X \\hat\\beta $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "This equation is called the \"normal\" equation. Now, if $X^T X$ is invertible, we have "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "$$\\hat\\beta = (X^T X )^{-1} X^T Y   $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "If $X^TX$ is not invertible, (which will happen if some of the explanatory variable can be computed as linear combinaison of the others), we can always take a pseudo inverse of $X^TX$, denoted for instance by $(X^TX)^+$, and compute the $\\hat\\beta$ with $\\hat\\beta = (X^T X )^{+} X^T Y $. \n",
      "\n",
      "A full description of the pseudo inverse (here, we assume the *Moore-Penrose* pseudo-inverse) is out of the scope here, but we can think of it as acting as the inverse of a matrix, such that :\n",
      "\n",
      "$X X^+ X = X$, and $X^+ X X^+ = X^+$, and both $X X^+$ and $X^+ X$ are symmetric.\n",
      "\n",
      "If $X^+$ is a the pseudo inverse of $X$, we have $\\hat{\\beta} = X^+ Y$ (and $(X^TX)^+X$ is also a pseudo inverse of X).\n",
      "\n",
      "In any case, we can compute our best estimates for $Y$ given the model $X$ with : $\\widehat{Y} = X \\hat\\beta$, and our estimated noise or residuals by $\\hat\\epsilon = Y - \\widehat{Y} = Y - X\\hat\\beta $. \n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Applying to our voxel data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x0 = np.asarray([1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1])\n",
      "x1 = np.ones(12)\n",
      "X = np.vstack((x0,x1)).T\n",
      "print X\n",
      "X_firstmodel = copy(X)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import linalg as lin\n",
      "\n",
      "# take only the first 12 values of the data - for \n",
      "# scale voxel data for better visual display; make its mean == 3.0\n",
      "Y = vdata[:12]  \n",
      "Y = (Y - Y.mean())/np.std(Y)  + 3.0\n",
      "vdata = Y\n",
      "\n",
      "pinvX   = lin.pinv(X)\n",
      "betah   = pinvX.dot(Y)\n",
      "Yfitted = X.dot(betah)\n",
      "resid   = Y - Yfitted\n",
      "print \"Y:\\n\", Y, \"\\nresid:\\n\", resid, \"\\nbetah:\\n\", betah\n",
      "print \"mean of Y: \", np.mean(Y), \"\\t mean of resid: \", np.mean(resid)\n",
      "\n",
      "# make this a little function as we will be reusing it :\n",
      "def glm(X,Y):\n",
      "    \"\"\" a simple GLM function returning the estimated parameters and residuals \"\"\"\n",
      "    betah   =  lin.pinv(X).dot(Y)\n",
      "    Yfitted =  X.dot(betah)\n",
      "    resid   =  Y - Yfitted\n",
      "    return betah, Yfitted, resid\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot the results\n",
      "x = range(12)\n",
      "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,4))\n",
      "ax1.plot(x, Y, 'r-', x, Yfitted, 'b-.')\n",
      "ax2.plot(x, resid, 'g--')\n",
      "\n",
      "# Again, we will want to reuse this code, let's make a tiny function:\n",
      "def plot_glm(Y, Yf, r):\n",
      "    x = range(Y.shape[0])\n",
      "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,4))\n",
      "    ax1.plot(x, Y, 'r-', x, Yf, 'b-.')\n",
      "    ax1.set_title('Y (red) Y fitted (blue)')\n",
      "    ax2.plot(x, r, 'g--')\n",
      "    ax2.set_title('residuals')\n",
      "\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Anoter way to compute the residuals is to compute the so called residual forming matrix $R$ :\n",
      "\n",
      "$$ \n",
      "\\hat\\epsilon = Y - \\widehat{Y} = Y - X \\hat\\beta = Y - X (X^T X)^+ X^T Y = (I_n -  X (X^T X)^+ X^T)Y = R Y\n",
      "$$\n",
      "\n",
      "where $I_n$ is the n by n identity matrix."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Interpreting our estimates of $\\beta$, changing the data and the model."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "At the look of our results, it seems that there's not much in the first regressor. We have $Y = 0.219   x_1 + 3.0$\n",
      "We can check that 1) the residuals have mean zero (they are \"uncorrelated with the constant x_1\") the mean of the Ys is equal to \\beta_1, which is the case because $x_0$ has mean zero. We could also check that the residuals are uncorrelated with $\\bf x_0$.\n",
      "\n",
      "While the first coefficient is small, especially with respect to the residuals, we still have to see whether this coefficient is \"significantly\" different from zero. This is the topic of our next section."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Making a slightly more general example"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "But before going into testing, let's make our example a little more general. Let's assume that our scans are divided in another 2 groups split, where the first 6 scans are of one condition (or group) and the last six ones of another. \n",
      "We can model this with a vector $\\bf x = [\\textrm{1 1 1 1 1 1 -1 -1 -1 -1 -1 -1}]^T$, which will model the difference between the first and the last 6 scans.\n",
      "Another solution is to include two regressors, one for the mean of the six first scans and one for the mean of the last six scans. Let's adopt this parametrisation of the model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vdata = vdata[:12]\n",
      "Y = vdata * 1.\n",
      "print mean(Y)\n",
      "x0 = np.asarray([1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1])\n",
      "x1 = np.hstack((np.ones(6),np.zeros(6)))\n",
      "x2 = np.hstack((np.zeros(6),np.ones(6)))\n",
      "x3 = np.ones(12)\n",
      "X = np.vstack((x0,x1,x2,x3)).T\n",
      "print X\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Let's add signal (== something that is in the model) in the data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Add some signal to our voxel, for the sake of the example:\n",
      "\n",
      "Y += 0.5*mean(Y)*x1 - 0.5*mean(Y)*x2\n",
      "\n",
      "print Y, mean(Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Let's fit our model for the new model and data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "beta, Yfitted, resid = glm(X,Y)\n",
      "print beta, mean(Y)\n",
      "# (beta[1]/2 + beta[2]/2 + beta[3])"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot the results\n",
      "plot_glm(Y, Yfitted, resid)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "What if there is signal not in the model ? "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Let's see what results we would have had with the first model (that doesnt have the regressor modelling the \"on off\" shape, but the new data that have this signal:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print X_firstmodel\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "glm_result = glm(X_firstmodel, Y)\n",
      "plot_glm(Y, glm_result[1], glm_result[2])\n",
      "print glm_result[0] # the betas"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "You may notice: the beta estimated with these new data (we have added the \"step\" function) are exactly the same than the ones estimated on the original data. Why ? Because we added in the Y something that is completely uncorrelated to the model X_firstmodel. No regressor in this first model can pick it up, therefore the betas are unchanged. \n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Changing the model $X$ by adding a regressor that is not in the model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "If we are adding a regressor $x_2$ to the model that is uncorrelated to each and every column of $X$, (ie adding $\\bf{x_2}$ with $\\bf{x_0}^t\\bf{x_2} = 0$, and $\\bf{x_1}^t\\bf{x_2} = 0)$, $\\bf{x_2}$ is in the third position $X_{i2}$, the $\\hat\\beta_0$ and $\\hat\\beta_1$ would be unchanged. Let's check:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x1_ = np.hstack((np.ones(6),-np.ones(6)))\n",
      "x2 = np.ones(12)\n",
      "X_ = np.vstack((x0,x1_,x2)).T\n",
      "print X_.T.dot(X_)\n",
      "glm_result = glm(X_, Y) # original data\n",
      "plot_glm(Y, glm_result[1], glm_result[2])\n",
      "print glm_result[0] # the betas"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print X_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "If there's some correlation between the regressors already in the model and the added regressor ? then the parameters $\\beta$ will change. But the critical piece of information is what is or is not in the model, it is its  \"flexibility\". \n",
      "\n",
      "What happens if we have too many things in the model ? \n",
      "\n",
      "Two cases \n",
      "\n",
      "    * Those things can capture some of the noise (the residual variance decrease)\n",
      "    * Those things cannot ... (the residual variance doesn't decrease)\n",
      "    \n",
      "In the first case, we will be **overfitting**. In the second case, we simply loose df\n",
      "\n",
      "Let's now see how we **test** the parameters that we have just estimated. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Testing the $\\hat\\beta$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "The model and the data can be displayed as little images: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f, (a1, a2, a3) = subplots(1, 3, figsize=(10,3))\n",
      "a1.imshow(Y[:,np.newaxis], interpolation='nearest', cmap='gray')\n",
      "a2.imshow(X, interpolation='nearest',cmap='gray')\n",
      "a3.imshow(resid[:,np.newaxis], interpolation='nearest',cmap='gray')\n",
      "titles = ['Y','X','e']\n",
      "for ix,a in enumerate([a1, a2, a3]):\n",
      "    a.set_xticklabels([])\n",
      "    a.set_title(titles[ix])\n",
      "    #a.set_yticklabels([])"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "There are two main statistics to understand with the GLM: the t- and the F- tests. First, we have to assume that our data $Y$, or equivalently the residuals $\\epsilon$, are normally distributed. We write $Y \\sim N(X\\beta, \\sigma^2)$, or, $\\epsilon \\sim N(0, \\sigma^2)$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "GLM and T-tests "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Most often the t-test is used to answer the question of whether a linear combinaison of the estimated parameters is zero. For instance, let's take $\\beta_1 - \\beta_2$ in the model above. This can be written with $c^T \\beta$ where $c^T = [0, 1, -1, 0]$. To see if this quantity is too high (or too small) to be expected if we only had noise in the data (and therefore that we can reject the hypothesis that it is zero), we compute the t statistics:\n",
      "\n",
      "$$\n",
      "t = \\frac{c^T \\hat\\beta}{\\sqrt{\\mathrm{var}(c^T \\hat\\beta)}}\n",
      "$$\n",
      "\n",
      "and it can be shown that t follows a student-distribution with degrees of freedom $df$ that we will compute below. \n",
      "\n",
      "Let's first compute $ \\mathrm{var}(c^T \\hat\\beta) $. We have\n",
      "\n",
      "$ \\mathrm{var}(c^T \\hat\\beta) = \\mathrm{var}(c^T X^+ Y) = c^T X^+ \\mathrm{var}(Y) X^{+T} c  = \\sigma^2 c^T X^+ X^{+T} c = \\sigma^2 c^T (X^T X)^+ c $. \n",
      "\n",
      "To compute the t-value, we need an estimate of $\\sigma^2$. We dont have access to the true $\\epsilon$, but we can get an estimate using the residuals $\\hat\\epsilon$. We therefore compute the **Residual Sum of Square** (RSS) with\n",
      "\n",
      "$$ \\textrm{RSS} = \\sum_{i=1}^{12} (Y_i - \\widehat{Y_i})^2 =  \\sum_{i=1}^{12} (Y_i - X_i \\hat\\beta)^2 = (Y - X\\hat\\beta)^T(Y - X\\hat\\beta) =  (Y - X X^+ Y)^T(Y - X X^+Y) = (Y^T R^T) R Y = Y^T R Y $$ \n",
      "\n",
      "where $X_i$ is the $i$th row of the matrix $X$. To obtain the estimate of $\\sigma$, we compute the *expectation* of RSS. Here, a couple of mathematical tricks are useful, involving the trace of a matrix (which is the sum of its diagonal elements). In short, playing with the trace and the expectation, we get : \n",
      "\n",
      "$$ E(\\textrm{RSS}) = E(Y^T R Y) =  E(tr(Y^T R Y)) = E(tr(R Y Y^T)) = tr(R E(YY^T)) = tr(\\sigma^2 R) = \\sigma^2 (n - \\textrm{rank}(X)) $$ \n",
      "\n",
      "It is not necessary to understand all the steps of this derivation, but it is interesting to understand the essence of what is shown above. In words, what this says is that if we were to compute the average value of the residuals sum of square, over many experiments, this sum of square is less that the sum of square of the true noise $\\epsilon$, and it depends on the rank of $X$. The rank of a $X$ is the minimal number of independant columns needed to construct $X$. Here, while we have 4 columns in $X$ but $\\textrm{rank}(X) = 3$ because we need only column 1, 2, and 3 (or 1, 2, 4) to reconstruct fully $X$. The more independant parameters we are estimating, the less degrees of freedom remains to estimate the variance of $\\epsilon$. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Following the equation above, and letting $p = \\textrm{rank}(X)$ we can estimate $\\sigma$ with $\\hat\\sigma$ where:\n",
      "\n",
      "$$\\hat\\sigma = \\textrm{RSS} / (n - p)$$\n",
      "\n",
      "This estimation is also called the **Mean Residual Sum of Square** (MRSS), and $\\hat\\sigma = \\textrm{MRSS} = \\textrm{RSS} / \\it{df} $  where $\\it{df}$ is the degrees of freedom $(n-p) = 12 - 3$, the number of observations (here, 12) minus the rank of the design matrix (here 3). In other word, this is the estimation of our noise.\n",
      "\n",
      "Coming back to our t-statistics, we can now calculate the **Standard Error** (SE) of $c^T \\hat\\beta$ with $\\textrm{SE} = \\sqrt{\\hat{\\sigma}^2 c^T (X^t X)^+ c} $\n",
      "\n",
      "The t test is then : $t = {c^T \\hat\\beta}/{\\textrm{SE}} $\n",
      "\n",
      "Clearly, for t to be big, we need the SE to be small, and / or $c^T \\hat\\beta$ to be large. Notice that the **SE** is composed of two elements: $\\hat{\\sigma}^2$ on the one hand, and  $c^T (X^t X)^+ c$ on the other hand. If the noise variance is high, then we have less chance to detect an effect. The second factor introduce how much the effect we are testing is correlated with other elements in the design matrix. If for instance, we are testing for $\\beta_0$ but we have a column in $X$ which is highly correlated with $\\bf{x_0}$, $c^T (X^t X)^+ c$ will be large and we also have less chance to detect the effect.  This led to some work for optimizing the design, once the questions of interest are formulated as contrast of the estimated parameters. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Let's compute this statistics in our example."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "betah, Yfitted, resid = glm(X,Y) # fit the model\n",
      "RSS = sum((Y - Yfitted)**2)\n",
      "MRSS = RSS/(len(Y) - linalg.matrix_rank(X))\n",
      "\n",
      "print \"\\nRSS method 1:\", RSS, \"\\nRSS method 2:\", sum(resid**2),  \\\n",
      "      \"\\nMRSS method 1:\", MRSS, \"\\nMRSS method 2:\", RSS/9 #checking\n",
      "print betah\n",
      "\n",
      "cT = np.asarray([0, 1, -1, 0])\n",
      "c  = cT[:,newaxis]\n",
      "XTX = np.linalg.pinv(X.T.dot(X))\n",
      "\n",
      "\n",
      "t_num = cT.dot(betah)\n",
      "SE = np.sqrt(MRSS* cT.dot(XTX).dot(c))\n",
      "t = t_num / SE\n",
      "print t"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "To see if this is significant, we have to see what is the probability of observing a t of 15.57 or larger under the null hypothesis. We first import some  utilities stat functions form scipy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from scipy.stats import t as tdist, norm as ndist"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t = 15.57\n",
      "pvalue = 1.0 - tdist.cdf(t,10)\n",
      "print \"t-value = \", t, \"p-value = \", pvalue\n",
      "\n",
      "# if it was 1.8 ?\n",
      "\n",
      "t = 1.8\n",
      "pvalue = 1.0 - tdist.cdf(t,10)\n",
      "print \"t-value = \", t, \"p-value = \", pvalue\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "GLM and F-tests "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "The simplest and generally most useful way of thinking of F test is to think as the test between two models: one which contains the regressor or factor that we want to test for (refered as the full model with design matrix $X$), and one which doesnt (the reduced model $X_0$). To test whether the model containing more columns is better, we compute the difference between the estimation of the noise variance between the models (variance estimated with $X$ versus variance estimated with $X_0$), normalized by the estimation of the noise variance under the full model. This is : \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "$$\n",
      "\\begin{eqnarray} \n",
      "F_{\\nu_1, \\nu_2} & = & \\frac{(\\hat\\epsilon_0^t \\hat\\epsilon_0 - \\hat\\epsilon^T\\hat\\epsilon)/ \\nu_{1} }{\\hat\\epsilon^T\\hat\\epsilon/\\nu_{2}} \\\\ \n",
      " & = & \\frac{(Y^TR_{X_0}Y - Y^TR_{X}Y)/ \\nu_{1} }{Y^TR_{X}Y/\\nu_{2}} \\\\ \n",
      " & = & \\frac{(Y^T(I-P_{X_0})Y - Y^T(I-P_{X})Y)/ \\nu_{1} }{Y^T(I- P_{X})Y/\\nu_{2}} \\\\ \n",
      "& = & \\frac{(\\textrm{SSR}(X_0) - \\textrm{SSR}(X))/\\nu_1}{\\textrm{SSR}(X)/\\nu_2}\n",
      "\\end{eqnarray}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "with $\\textrm{SSR}(X)$, $\\textrm{SSR}(X_0)$, the sum of squares for error of models $X, X_0$\n",
      "respectively, and:\n",
      "\n",
      "$$\n",
      "\\begin{eqnarray} \n",
      "\\nu_{1} & =  & \\textrm{tr}(P_X - P_{X_0}) = \\textrm{tr}(R_0 - R_X) \\\\ \n",
      "    \\nu_{2} & = & \\textrm{tr}(I - P_X) = \\textrm{tr}(R_X) \n",
      "\\end{eqnarray}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "It can be shown that under the hypothesis that our true model is $X_0$, this follows an $F$ distribution with $\\nu_1,nu_2$ degrees of freedom. Let's take an example with the previous model and implement this with code. Say for instance that we are interested to know wheter there is an effect of the step function in the model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X0 = X[:,[0,3]]\n",
      "betah, Yfitted, resid = glm(X,Y)\n",
      "betah0, Yfitted0, resid0 = glm(X0,Y)\n",
      "PX = X.dot(lin.pinv(X))\n",
      "RX = np.eye(X.shape[0]) - PX\n",
      "PX0 = X0.dot(lin.pinv(X0))\n",
      "\n",
      "F_num = (sum(resid0**2) - sum(resid**2))\n",
      "nu1 = np.trace(PX - PX0)\n",
      "F_den = sum(resid**2)\n",
      "nu2 = np.trace(RX)\n",
      "F = (F_num/nu1)/(F_den/nu2)\n",
      "print F, nu1, nu2, F_num, F_den"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.stats import f as Fdist\n",
      "Fdist.sf(F,nu1,nu2)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Regression versus correlation : how can things so similar be so different?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, let's make a little function for testing our "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def t_test(betah, resid, X):\n",
      "    \"\"\" \n",
      "    test the parameters betah one by one \n",
      "    \n",
      "    betah : (p, 1) estimated parameters\n",
      "    resid : (n, 1) estimated residuals\n",
      "    X : design matrix\n",
      "    \"\"\"\n",
      "\n",
      "    RSS = sum((resid)**2)\n",
      "    n = resid.shape[0]\n",
      "    q = np.linalg.matrix_rank(X)\n",
      "    df = n-q\n",
      "    MRSS = RSS/df\n",
      "    \n",
      "    XTX = np.linalg.pinv(X.T.dot(X))\n",
      "\n",
      "    tval = np.zeros(betah.shape)\n",
      "    pval = np.zeros(betah.shape)\n",
      "\n",
      "\n",
      "    for (idx, beta) in enumerate(betah):\n",
      "        c = zeros(betah.shape)\n",
      "        c[idx] = 1\n",
      "        t_num = c.T.dot(betah)\n",
      "        SE = np.sqrt(MRSS* c.T.dot(XTX).dot(c))\n",
      "        tval[idx] = t_num / SE\n",
      "   \n",
      "        pval[idx] = 1.0 - tdist.cdf(tval[idx], df)\n",
      "    \n",
      "    return tval, pval\n",
      "\n",
      "#print t_test(betah, Yfitted, resid)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n = 20\n",
      "x = np.random.randn(n,1)\n",
      "X  = np.hstack((x, np.ones(x.shape)))\n",
      "print X.shape\n",
      "m_y = 0 \n",
      "m_y = 3.14\n",
      "\n",
      "e = np.random.randn(n,1)\n",
      "y = 1.4 * x + m_y + e\n",
      "plt.plot(x,y,'o')\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What is the estimated beta ?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "betah, Yfitted, resid = glm(X,y)\n",
      "#print Yfitted.shape\n",
      "t, p =  t_test(betah, resid, X)\n",
      "print betah\n",
      "print t, \" p = \", p"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# do this the other way :\n",
      "\n",
      "X2 =  np.hstack((y - m_y, np.ones(x.shape)))\n",
      "y2 = x\n",
      "betah2, Yfitted2, resid2 = glm(X2, y2)\n",
      "print \"compare the slopes : \", 1./betah2[0], betah[0]\n",
      "print \"compare the residual variance : \", resid.T.dot(resid), resid2.T.dot(resid2)\n",
      "t2, p2 =  t_test(betah2, resid2, X2)\n",
      "\n",
      "print \"betah2 : \", betah2[:,0]\n",
      "print \"compare t values : \\n\", np.hstack((t, t2))\n",
      "print \"compare p values : \\n\", np.hstack((p, p2))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##But why is that ?##\n",
      "\n",
      "$y = x\\beta_0 + \\mu + \\epsilon$\n",
      "\n",
      "compared to \n",
      "\n",
      "$x = y/\\beta_0 - \\mu/\\beta_0 - \\epsilon/\\beta_0$\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Fisher z transform of correlation coefficient:\n",
      "\n",
      "def fisher_transf(rho):\n",
      "    \"\"\" take a coefficient of correlation and z transform it \n",
      "        see en.wikipedia.org/wiki/Fisher_transformation\n",
      "    \"\"\"\n",
      "    return (0.5 * np.log((1. + rho) / (1. - rho)))\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corr = np.corrcoef(x[:,0],y[:,0])\n",
      "z = fisher_transf(corr[0,1])\n",
      "\n",
      "p_fisher = 1. - ndist.cdf(z, 0, 1./np.sqrt(n-3))\n",
      "print p_fisher\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "GLM over the brain volume"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# An example of a t-test with 20 \"voxels\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# using nipy\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from nipy.labs.viz import plot_map, cm\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nipy.modalities.fmri.glm import FMRILinearModel\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}