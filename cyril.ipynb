{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A notebook for thoughts about Cyril Pernet's article on GLM misconceptions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import numpy.linalg as npl\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.mlab as mlab"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Import statistical distributions from scipy:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.stats import t as t_dist, f as f_dist, gamma"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A routine to scale the design matrix for display:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def scale_design_mtx(X):\n",
      "    \"\"\"utility to scale the design matrix for display\n",
      "\n",
      "    This scales the columns to their own range so we can see the variations \n",
      "    across the column for all the columns, regardless of the scaling of the \n",
      "    column.\n",
      "    \"\"\"\n",
      "    mi, ma = X.min(axis=0), X.max(axis=0)\n",
      "    col_neq = (ma - mi) > 1.e-8\n",
      "    Xs = np.ones_like(X)\n",
      "    mi = mi[col_neq]\n",
      "    ma = ma[col_neq]\n",
      "    Xs[:,col_neq] = (X[:,col_neq] - mi)/(ma - mi)\n",
      "    return Xs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Display the design matrix nicely:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def show_design(X, design_title, **kwargs):\n",
      "    \"\"\" Show the design matrix nicely \"\"\"\n",
      "    plt.figure()\n",
      "    plt.gray() # Gray colormap\n",
      "    plt.imshow(scale_design_mtx(X), interpolation='nearest', **kwargs)\n",
      "    plt.title(design_title)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Very simple t statistic from contrast and Ordinary Least Squares fit:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def fit_ols(Y, X):\n",
      "    \"\"\" betas, fitted data, and residuals from OLS linear fit.\n",
      "    \n",
      "    This is OLS estimation; we assume the errors to have independent\n",
      "    and identical normal distributions around zero for each $i$ in \n",
      "    $\\Epsilon_i$ (i.i.d)\n",
      "    \"\"\"\n",
      "    Y = np.asarray(Y)\n",
      "    X = np.asarray(X)\n",
      "    # Get the estimated betas\n",
      "    betah   =  npl.pinv(X).dot(Y)\n",
      "    fitted =  X.dot(betah)\n",
      "    resid   =  Y - fitted\n",
      "    return betah, fitted, resid"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def t_stat(Y, X, C):\n",
      "    \"\"\" betas, t statistic and significance test given data, design matrix, contrast\n",
      "    \n",
      "    Ordinary least squares estimation - see `fit_ols` function.\n",
      "    \"\"\"\n",
      "    Y = np.asarray(Y)\n",
      "    X = np.asarray(X)\n",
      "    C = np.atleast_2d(C)\n",
      "    # Calculate the parameters\n",
      "    B, fitted, resid = fit_ols(Y, X)\n",
      "    # Residual sum of squares\n",
      "    RSS   = (resid**2).sum(axis=0)\n",
      "    # Degrees of freedom - number of observations - number of fitted parameters\n",
      "    df =  X.shape[0] - npl.matrix_rank(X)\n",
      "    # Mean residual sum of squares\n",
      "    MRSS  = RSS / df\n",
      "    # Standard error of contrast estimate C.dot(B)\n",
      "    SE    = np.sqrt(MRSS * C.dot(npl.pinv(X.T.dot(X)).dot(C.T)))\n",
      "    t     = C.dot(B)/SE\n",
      "    ltp   = t_dist(df).cdf(t) # lower tail p\n",
      "    p = 1 - ltp # upper tail p\n",
      "    return B, t, df, p"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Make the simulated data from C.P.s figure 2 (was figure 1):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a_block = np.array([-0.1, 0, 0.1])\n",
      "baseline = 10\n",
      "activation = 11\n",
      "on_off = np.hstack((a_block + baseline, a_block + activation))\n",
      "n_on_off = 3\n",
      "e = np.tile(a_block, (2 * n_on_off,))\n",
      "y = np.tile(on_off, (n_on_off,))\n",
      "plt.plot(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x_on = np.hstack((np.zeros(len(a_block)), np.ones(len(a_block))))\n",
      "x_off = 1 - x_on\n",
      "X_over_part = np.column_stack((x_on, x_off, np.ones_like(x_on)))\n",
      "X_over = np.tile(X_over_part, (n_on_off, 1))\n",
      "show_design(X_over, 'over parametrized')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_well_part = np.column_stack((x_on, np.ones_like(x_on)))\n",
      "X_well = np.tile(X_well_part, (n_on_off, 1))\n",
      "show_design(X_well, 'well parametrized')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_times2 = X_well.copy()\n",
      "X_times2[:, 0] *= 2\n",
      "show_design(X_times2, 'times 2')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "B_over = npl.pinv(X_over).dot(y)\n",
      "B_over"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Print betas, t statistic, df, p value for different designs, contrasts\n",
      "print(t_stat(y, X_over, [1, 0, 0]))\n",
      "print(t_stat(y, X_well, [1, 0]))\n",
      "print(t_stat(y, X_times2, [1, 0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The t statistic on the over-parametrized design does not relflect the estimability of the contrast:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def is_estimable(X, C):\n",
      "    \"\"\" Is a contrast C estimable on a design X\n",
      "    \n",
      "    To be estimable, the contrast needs to be orthogonal to the null-space of X.\n",
      "    The null space of X is all the vectors $k$ such that Xk = 0.\n",
      "    If $k$ is a vector in the null-space of X, then:\n",
      "    \n",
      "    Y = X B + e\n",
      "    \n",
      "    and\n",
      "    \n",
      "    Y = X (B + k) + e\n",
      "    \n",
      "    give the same fit (X B == X (B + k)).\n",
      "    \n",
      "    The null space of X is the orthogonal complement of the row-space of X.  So C\n",
      "    has to be in the row space of X.\n",
      "    \n",
      "    A less mathematical way of seeing this is that the information we will need to form\n",
      "    our betas is made of linear combinations of the rows of X, because the betas are\n",
      "    constructed from Xt Y. \n",
      "    \"\"\"\n",
      "    C = np.atleast_2d(C)\n",
      "    rankX = npl.matrix_rank(X)\n",
      "    return rankX == npl.matrix_rank(np.vstack((C, X)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "is_estimable(X_over, [1, 0, 0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "is_estimable(X_over, [1, -1, 0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In our case, the null space of $\\mathbf{X}$ is:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sympy\n",
      "X = sympy.Matrix(X_over)\n",
      "sympy.pretty_print(X.nullspace())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Any contrast vector not orthogonal to this null space is not estimable. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Another way of seeing this is to get a minimal set of vectors that \"span\" the rows of X. These can be found using the svd :"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[u, s, vt] = npl.svd(X_over, full_matrices=False)\n",
      "tol = s.max() * max(X_over.shape) * np.finfo(s.dtype).eps\n",
      "nz = np.where(s > tol)[0]\n",
      "print nz\n",
      "vt[np.abs(vt) < tol] = 0\n",
      "print vt.T[:,nz]\n",
      "print \"\\nNormalize the columns \\n\"\n",
      "print vt.T[:,nz].dot(np.diagflat([1./vt[0,0], 1./vt[1,1]]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, any contrast that is made of these two is estimable. In particular, what do you think the first column would test for?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Compare to an F test:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def f_stat(Y, X, col):\n",
      "    \"\"\" betas, F statistic and significance test given data, design and column to test\n",
      "    \n",
      "    We do the F statistic long hand, by fitting the model with and without the column to test.\n",
      "    \n",
      "    This is OLS estimation; we assume the errors to have independent\n",
      "    and identical normal distributions around zero for each $i$ in \n",
      "    $\\Epsilon_i$ (i.i.d).\n",
      "    \"\"\"\n",
      "    Y = np.asarray(Y)\n",
      "    full_X = np.asarray(X)\n",
      "    # Delete column to make reduced design\n",
      "    reduced_X = np.delete(full_X, col, 1)\n",
      "    # fit both designs\n",
      "    _, _, full_resid = fit_ols(Y, full_X)\n",
      "    _, _, reduced_resid = fit_ols(Y, reduced_X)\n",
      "    n_obs = Y.shape[0]\n",
      "    # Number of parameters used by each design\n",
      "    n_p_full = npl.matrix_rank(full_X)\n",
      "    n_p_reduced = npl.matrix_rank(reduced_X)\n",
      "    # Extra sum of squares\n",
      "    full_SS = (full_resid ** 2).sum()\n",
      "    reduced_SS = (reduced_resid ** 2).sum()\n",
      "    extra_SS = (reduced_SS - full_SS) / (n_p_full - n_p_reduced)\n",
      "    # F statistic\n",
      "    F = extra_SS / (full_SS / (n_obs - n_p_full))\n",
      "    ltp   = f_dist((n_p_full - n_p_reduced), n_obs - n_p_full).cdf(F) # lower tail p\n",
      "    p = 1 - ltp # upper tail p\n",
      "    return F, p"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f_stat(y, X_well, 0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What happens with the over-parametrized design?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f_stat(y, X_over, 0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Was that what you were expecting?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Correlated regressors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Remind ourselves about the t statistic on the over-parametrized design\n",
      "print(t_stat(y, X_over, [0, 0, 1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Add a tiny bit of noise to the over-parametrized design\n",
      "X_over_tweaked = X_over.copy()\n",
      "X_over_tweaked[0, 2] = 1e-14\n",
      "print(t_stat(y, X_over_tweaked, [0, 0, 1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The general case of correlated regressors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# first a quick function to yield two correlated regressor with correlation c\n",
      "def correlated(c, n=20):\n",
      "    np.random.seed(42) # To get predictable random numbers\n",
      "    # generate two uncorrelated regressors\n",
      "    Y = np.random.normal(0,1,(20,2))\n",
      "    #make them correlated with mixing matrix M:\n",
      "    #we want a correlation c between Y[0] and Y[1], with unit variance\n",
      "    #this is the mixing matrix needed:\n",
      "    c1 = np.sqrt( (1 + np.sqrt(1-c*c))/2 ) \n",
      "    c2 = .5 * c / c1\n",
      "    M = np.asarray([[c1, c2],[c2, c1]])\n",
      "    Yc = Y.dot(M)     # Make Yc = MY\n",
      "    return Yc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n = 20\n",
      "Xc = correlated(.2, n)\n",
      "x1c = Xc[:,0]; x2c = Xc[:,1]; \n",
      "print np.corrcoef(Xc.T)[0,1]\n",
      "\n",
      "y_corr = x1c + 1.5*np.random.normal(size=n) + 10\n",
      "#x1c = np.random.normal(size=n)\n",
      "#x2c = np.random.normal(size=n)\n",
      "X_corr = np.column_stack((x1c, x2c, np.ones_like(x2c)))\n",
      "\n",
      "C = np.array([1, 0, 0]) # The contrast\n",
      "t_stat(y_corr, X_corr, C)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "XtX = X_corr.T.dot(X_corr)\n",
      "XtX"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "npl.pinv(XtX)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "C.dot(npl.pinv(XtX)).dot(C)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n=20\n",
      "x2c_corr = np.linspace(0.1, 0.999, 15)\n",
      "dtype = np.dtype(dict(names=['t', 'mss', 'b0', 'b1', 'b2', 'xvar', 'df'],\n",
      "                      formats=['f8'] * 7))\n",
      "res = np.zeros(len(x2c_corr), dtype=dtype)\n",
      "\n",
      "for i, x2c_corr in enumerate(x2c_corr):\n",
      "    Xc = correlated(x2c_corr, n)\n",
      "    # new_x1c = (1 - x2c_prop) * x1c + x2c_prop * x2c\n",
      "    new_x1c = Xc[:,0]; x2c = Xc[:,1];\n",
      "    # X_prop = np.column_stack((new_x1c, x2c, np.ones_like(x2c)))\n",
      "    X_corr = np.column_stack((new_x1c, x2c, np.ones_like(x2c)))\n",
      "    y_corr = x1c + 1.5*np.random.normal(size=n) + 10\n",
      "\n",
      "    betas, res[i]['t'], res[i]['df'], p = t_stat(y_corr, X_corr, C)\n",
      "    res[i]['b0'], res[i]['b1'], res[i]['b2'] = betas[:]\n",
      "    fitted = X_corr.dot(betas)\n",
      "    res[i]['mss'] = ((y_corr - fitted)**2).sum() / res[i]['df']\n",
      "    res[i]['xvar'] = C.dot(npl.pinv(X_corr.T.dot(X_corr)).dot(C))\n",
      "print(mlab.rec2txt(res))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "HRF and temporal derivatives"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now consider a design convolved with an HRF:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def spm_hrf(t):\n",
      "    \"\"\" Return SPM hrf sampled at times `t`\n",
      "    \"\"\"\n",
      "    # gamma.pdf only defined for t > 0\n",
      "    hrf = np.zeros_like(t, dtype=np.float)\n",
      "    hrf[t > 0] = gamma.pdf(t[t > 0], 6, 0, 1) - gamma.pdf(t[t > 0], 16, 0, 1) / 6.\n",
      "    return hrf / np.sum(hrf)\n",
      "\n",
      "def spm_hrf_d(t):\n",
      "    \"\"\" Return temporal derivative of SPM HRF sampled at times `t`\n",
      "    \"\"\"\n",
      "    # This is what spm does!\n",
      "    return spm_hrf(t) - spm_hrf(t - 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t = np.arange(24)\n",
      "plt.plot(t, spm_hrf(t), label='HRF')\n",
      "plt.plot(t, spm_hrf_d(t), label='TD')\n",
      "plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The design without HRF convolution"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "block_length = 24\n",
      "on_off = np.hstack((np.zeros(block_length), np.ones(block_length)))\n",
      "off_on = 1 - on_off\n",
      "X_hrf_over = np.column_stack((on_off, off_on, np.ones_like(on_off)))\n",
      "X_hrf_over = np.tile(X_hrf_over, (10, 1))\n",
      "show_design(X_hrf_over, 'before convolution', aspect=0.01)\n",
      "S = npl.svd(X_hrf_over, compute_uv=False)\n",
      "print('Smallest singular value (if close to 0 matrix may be rank deficient)')\n",
      "print(S.min())\n",
      "# Is -1 -1 1 still in the null space? Yes.\n",
      "np.allclose(X_hrf_over.dot([-1, -1, 1]), 0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Design with convolution:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hrf = spm_hrf(t)\n",
      "x0 = np.convolve(hrf, X_hrf_over[:, 0], mode='same')\n",
      "x1 = np.convolve(hrf, X_hrf_over[:, 1], mode='same')\n",
      "X_conv_over = np.column_stack((x0, x1, np.ones_like(x0)))\n",
      "show_design(X_conv_over, 'Convolved design', aspect=0.01)\n",
      "S = npl.svd(X_conv_over, compute_uv=False)\n",
      "print('Smallest singular value (if close to 0 matrix may be rank deficient)')\n",
      "print(S.min())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that the matrix with convolved columns is no longer rank deficient, and the vector:\n",
      "\n",
      "$$\n",
      "\\left[\\begin{matrix}-1\\\\-1\\\\1\\end{matrix}\\right]\n",
      "$$\n",
      "\n",
      "is no longer in the null space (because there is no null space for the convolved matrix)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Is -1 -1 1 still in the null space?\n",
      "np.allclose(X_conv_over.dot([-1, -1, 1]), 0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "OK, let's make some data based on the design:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x0_hrf = X_conv_over[:, 0]\n",
      "X_conv_well = np.column_stack((x0_hrf, np.ones_like(x0)))\n",
      "y = (x0_hrf - x0_hrf.mean()) * 1 + 10"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "npl.pinv(X_conv_well).dot(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We don't reconstruct 10 correctly because the ``x0`` regressor does not have mean 0:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x0_hrf.mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_conv_orth = np.column_stack((x0_hrf - x0_hrf.mean(), np.ones_like(x0)))\n",
      "npl.pinv(X_conv_orth).dot(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Make a temporal derivative:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dhrf = spm_hrf_d(np.arange(24))\n",
      "x0_dhrf = np.convolve(dhrf, x0, mode='same')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The derivative is not orthogonal to the HRF, after convolution:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Basis functions, before convolution with block - nearly orthogonal\n",
      "hrf.dot(dhrf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# After convolution with block - not orthogonal\n",
      "x0_hrf.dot(x0_dhrf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can orthogonalize using the same mechamism as we use for the fit:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def orth_x_wrt_y(x, y):\n",
      "    \"\"\" Orthonalize vector `x` with respect to matrix `y`\n",
      "    \"\"\"\n",
      "    y_in_x_beta = npl.pinv(y).dot(x)\n",
      "    y_in_x = y.dot(y_in_x_beta)\n",
      "    return x - y_in_x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x0_dhrf_orth = orth_x_wrt_y(x0_dhrf, X_conv_orth)\n",
      "x0_dhrf_orth.T.dot(X_conv_orth)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x0_hrf.dot(x0_dhrf_orth)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As we know, adding an orthogonal regressor to the whole the rest of the design cannot affect the fit for other regressors:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_dconv_orth = np.column_stack((X_conv_orth[:, 0], x0_dhrf_orth, np.ones_like(x0)))\n",
      "npl.pinv(X_dconv_orth).dot(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With or without noise:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ye = y + np.random.normal(size=(y.shape))\n",
      "print(npl.pinv(X_conv_orth).dot(ye))\n",
      "print(npl.pinv(X_dconv_orth).dot(ye))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "But - if the new regressor is not orthogonal to the whole of the rest of the design, the fit can change, even for regressors that *are* orthogonal to the new regressor:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Design in which hrf regressor not orthogonal to constant\n",
      "partial_orthed = orth_x_wrt_y(x0_dhrf, x0_hrf[:, None])\n",
      "X_colinear = np.column_stack((x0_hrf, np.ones_like(x0_hrf)))\n",
      "X_mixed = np.column_stack((x0_hrf, partial_orthed, np.ones_like(x0_hrf)))\n",
      "X_mixed.T.dot(X_mixed)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So the second temporal derivative regressor is orthogonal to the first HRF regressor, but not orthogonal to the constant.  Estimating:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(npl.pinv(X_colinear).dot(ye))\n",
      "print(npl.pinv(X_mixed).dot(ye))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Estimation in the presence of noise caused the parameter for the first HRF regressor to change, even though the added regressor is orthogonal to the first HRF regressor."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}