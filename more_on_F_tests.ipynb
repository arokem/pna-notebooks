{
 "metadata": {
  "name": "more_on_F_tests"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "The hidden truth on F-tests and contrasts "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We know that the traditional way of looking at an F-contrast is to think of two models : a full model $X$, and a reduced model $X_0$, and compute the sum of square of the residuals under these two models. We also know that we can express the numerator of an F test with a contrast that impose a constraint on the parameters of the model. This short notebook is to make clear the relation between these two formulations, and therefore being able to test unusual hypotheses more easily. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.linalg as lin \n",
      "import scipy.stats as sst\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 96
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n = 30\n",
      "sig = 1.\n",
      "Y = np.random.randn(n, sig)\n",
      "\n",
      "# make a design of 1 factor 3 levels\n",
      "oneway_anov = np.kron(np.eye(3), ones((10,1)))\n",
      "\n",
      "#add a trend?\n",
      "trend = np.arange(0,n).reshape(n,1)\n",
      "\n",
      "# add an intercept\n",
      "intercept = np.ones((n,1))\n",
      "\n",
      "X = np.hstack((oneway_anov, intercept))\n",
      "\n",
      "n, p = X.shape\n",
      "\n",
      "csignal = np.array([1., -1., 0, 3.]).reshape(4,1)\n",
      "signal = X.dot(csignal)\n",
      "Y +=  signal\n",
      "\n",
      "print np.hstack((Y, X))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 3.9943518   1.          0.          0.          1.        ]\n",
        " [ 2.44220583  1.          0.          0.          1.        ]\n",
        " [ 3.24520654  1.          0.          0.          1.        ]\n",
        " [ 3.64081319  1.          0.          0.          1.        ]\n",
        " [ 4.46484033  1.          0.          0.          1.        ]\n",
        " [ 3.89383629  1.          0.          0.          1.        ]\n",
        " [ 4.19231461  1.          0.          0.          1.        ]\n",
        " [ 4.72630883  1.          0.          0.          1.        ]\n",
        " [ 5.37665183  1.          0.          0.          1.        ]\n",
        " [ 4.84416339  1.          0.          0.          1.        ]\n",
        " [ 1.94976513  0.          1.          0.          1.        ]\n",
        " [ 2.36354817  0.          1.          0.          1.        ]\n",
        " [ 0.26766017  0.          1.          0.          1.        ]\n",
        " [ 0.26305283  0.          1.          0.          1.        ]\n",
        " [ 1.56965414  0.          1.          0.          1.        ]\n",
        " [ 1.0269296   0.          1.          0.          1.        ]\n",
        " [ 0.77024914  0.          1.          0.          1.        ]\n",
        " [ 0.55208031  0.          1.          0.          1.        ]\n",
        " [ 2.61641458  0.          1.          0.          1.        ]\n",
        " [ 0.82008908  0.          1.          0.          1.        ]\n",
        " [ 2.77217748  0.          0.          1.          1.        ]\n",
        " [ 2.95074527  0.          0.          1.          1.        ]\n",
        " [ 3.87632139  0.          0.          1.          1.        ]\n",
        " [ 2.27354835  0.          0.          1.          1.        ]\n",
        " [ 1.27890485  0.          0.          1.          1.        ]\n",
        " [ 2.00451691  0.          0.          1.          1.        ]\n",
        " [ 2.62793957  0.          0.          1.          1.        ]\n",
        " [ 3.08104567  0.          0.          1.          1.        ]\n",
        " [ 2.54475458  0.          0.          1.          1.        ]\n",
        " [ 1.76884071  0.          0.          1.          1.        ]]\n"
       ]
      }
     ],
     "prompt_number": 97
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "F test with a known reduced model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's imaging that we know the reduced model: we'd like to test what is in $X$ but not in $X_0$.\n",
      "\n",
      "We define the orthogonal projector onto $X$ as $P_X = X X^-$, where $X^-$ is the moore-penrose pseudo inverse.\n",
      "\n",
      "\\begin{eqnarray} \n",
      "%\\label{equ:F} \n",
      "  F_{\\nu_1, \\nu_2} & = & \\frac{(Y^T(I-P_{X_0})Y - Y^T(I-P_{X})Y)/ \\nu_{1} }{Y^T(I-P_{X})Y/\\nu_{2}} \\\\ \n",
      "  \t\t& = & \\frac{(SSR(X_0) - SSR(X))/\\nu_1}{SSR(X)/\\nu_2}\n",
      "\\end{eqnarray}\n",
      "\n",
      "with $SSR(X)$, (resp. $SSR(X_0)$) the sum of square for error of model $X$ (resp. $X_0$), and\n",
      "\n",
      "\\begin{eqnarray} \n",
      "  \\nu_{1} & =  & tr(P_X - P_{X_0}) = tr(R_0 - R_X) \\\\ \\nonumber \\nu_{2} & = & tr(I - P_X) = tr(R_X) \n",
      "\\end{eqnarray}\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numpy.linalg import matrix_rank"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 98
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# lets do an F-test : \n",
      "\n",
      "def F_test(Y, X, L):\n",
      "    '''\n",
      "    Y: data : numpy array\n",
      "    X: design matrix : numpy array \n",
      "    L: contrast : numpy array\n",
      "    '''\n",
      "    X0 = X.dot(L)\n",
      "    n, _ = X.shape\n",
      "    p0 = matrix_rank(X0)\n",
      "    p = matrix_rank(X)\n",
      "    \n",
      "    nu1 = p-p0\n",
      "    nu2 = n-p\n",
      "\n",
      "    SSR_num = (Y.T.dot(proj(X, Y)) - Y.T.dot(proj(X0, Y)))\n",
      "    numF = SSR_num/nu1\n",
      "    print '    ', SSR_num, p, p0\n",
      "    SSR_den = (Y.T.dot(R_proj(X, Y)))\n",
      "    denF = SSR_den/nu2\n",
      "    print '    ', SSR_den, n, p\n",
      "    \n",
      "    return numF/denF, nu1, nu2\n",
      "\n",
      "    \n",
      "def proj(X, Y):\n",
      "    # project onto the space of X\n",
      "    #print matrix_rank(X)\n",
      "    [u, s, vt] = lin.svd(X, full_matrices=False)\n",
      "    return u.dot(u.T.dot(Y))\n",
      "    \n",
      "def R_proj(X, Y):\n",
      "    # project onto the space orthogonal to X (RY) \n",
      "    return Y - proj(X, Y)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 99
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Understanding the numerator : Matthew's elegant and simple proposal"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At line $i$ :\n",
      "\n",
      "$$\n",
      "\\begin{eqnarray}\n",
      "\\epsilon_{0i}^2 - \\epsilon_i^2 &=& (Y - \\mu)^2  - (Y - \\beta_{i} - \\mu)^2 \\\\\n",
      "                            &=& \\epsilon_{0i}^2 - (\\epsilon_{0i} - \\beta_{i})^2 \\\\\n",
      "                            &=& \\beta_i^2 + 2\\epsilon_{0i}\\beta_{i}\n",
      "\\end{eqnarray}\n",
      "$$\n",
      "\n",
      "Summing these, we see that  $\\sum_i\\epsilon_{0i}\\beta_{i} = 0$ since each column of X must be decorrelated from the noise $\\epsilon_0$ "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A more general version of this can be written in the following way, with a small bit of matrix algebra:\n",
      "\n",
      "\n",
      "$$\n",
      "\\begin{eqnarray}\n",
      "SSR(X_0) - SSR(X) & = & (Y^T(I-P_{X_0})Y - Y^T(I-P_{X})Y)  \\\\ \n",
      "                & = & Y^TY - YP_{X_0}Y - Y^TY + YP_{X}Y \\\\\n",
      "                & = & Y^TP_{X}Y - Y^TP_{X_0}Y\n",
      "\\end{eqnarray}\n",
      "$$\n",
      "\n",
      "But $Y^TP_{X}Y$ is also $Y^TP_{X}P_{X}Y = Y^TX(X^TX)^{-1}X^T X(X^TX)^{-1}X^T Y =  \\beta^T X^T X \\beta$\n",
      "\n",
      "The first equality is because $P_{X} = P_{X}P_{X}$, the second by replacing  $P_X$ by $X(X^TX)^{-1}X^T $. If we can partition $X$ in $[X_t X_0]$ where $X_t$ is the tested part (with parameters $\\beta_t$), and if they are orthogonal, then   \n",
      "\n",
      "$$\n",
      "\\beta^T X^T X \\beta = \\beta_0^T X_0^T X_0 \\beta_0 + \\beta_t^T X_t^T X_t \\beta_t\n",
      "$$\n",
      "\n",
      "and therefore\n",
      "$$\n",
      "SSR(X_0) - SSR(X)  = \\beta_t^T X_t^T X_t \\beta_t\n",
      "$$\n",
      "\n",
      "which corresponds to the sum of square of the means, weighted by the number of observation (see this by doing the dot product of $X_t \\beta_t$ by itself. Note what is $ X_t \\beta_t$ : the \"means\". "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "L = np.eye(p)[:,3:]\n",
      "X0 = X.dot(L)\n",
      "# print X0\n",
      "(F, n1, n2) = F_test(Y, X, L)\n",
      "print (F, n1, n2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "     [[ 41.25350126]] 3 1\n",
        "     [[ 17.70875551]] 30 3\n",
        "(array([[ 31.44897825]]), 2, 27)\n"
       ]
      }
     ],
     "prompt_number": 100
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# divide X in Xt and X0 :\n",
      "X0 = X[:,[3]]\n",
      "# orthogonolize the three first columns of X : this is what we want to test\n",
      "Xt = R_proj(X0, X[:,:3])\n",
      "#print Xt\n",
      "Xall = np.hstack((Xt, X0))\n",
      "\n",
      "betah = np.linalg.pinv(Xall).dot(Y)\n",
      "betah_t = betah[:3,:]\n",
      "print betah_t\n",
      "\n",
      "tmp = Xt.dot(betah_t)\n",
      "SSRXt = tmp.T.dot(tmp)\n",
      "print SSRXt\n",
      "\n",
      "print  Y.T.dot(proj(Xall, Y)) - Y.T.dot(proj(X0, Y))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 1.47543824]\n",
        " [-1.3866867 ]\n",
        " [-0.08875154]]\n",
        "[[ 41.07694861]]\n",
        "[[ 42.53458219]]\n"
       ]
      }
     ],
     "prompt_number": 101
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.stats import f as Fdist\n",
      "Fdist.sf(F,n1,n2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 102,
       "text": [
        "array([[  8.86723563e-08]])"
       ]
      }
     ],
     "prompt_number": 102
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We are close. But there seems to be some numerical problem. Let's see what a non degenerate design matrix does:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# recreate the original\n",
      "X = np.hstack((oneway_anov, intercept))\n",
      "\n",
      "L = np.array([[1., -1., 0, 0],[0., 1., -1., 0]]).T\n",
      "\n",
      "Xt = X.dot(L)\n",
      "X0 = X[:,[3]]\n",
      "Xall = np.hstack((Xt, X0))\n",
      "\n",
      "betah = np.linalg.pinv(Xall).dot(Y)\n",
      "\n",
      "print Xall\n",
      "print Xall.T.dot(Xall)\n",
      "\n",
      "betah_t = betah[:2,:]\n",
      "print betah_t\n",
      "\n",
      "tmp = Xt.dot(betah_t)\n",
      "SSRXt = tmp.T.dot(tmp)\n",
      "print SSRXt\n",
      "\n",
      "print  Y.T.dot(proj(Xall, Y)) - Y.T.dot(proj(X0, Y))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 1.  0.  1.]\n",
        " [ 1.  0.  1.]\n",
        " [ 1.  0.  1.]\n",
        " [ 1.  0.  1.]\n",
        " [ 1.  0.  1.]\n",
        " [ 1.  0.  1.]\n",
        " [ 1.  0.  1.]\n",
        " [ 1.  0.  1.]\n",
        " [ 1.  0.  1.]\n",
        " [ 1.  0.  1.]\n",
        " [-1.  1.  1.]\n",
        " [-1.  1.  1.]\n",
        " [-1.  1.  1.]\n",
        " [-1.  1.  1.]\n",
        " [-1.  1.  1.]\n",
        " [-1.  1.  1.]\n",
        " [-1.  1.  1.]\n",
        " [-1.  1.  1.]\n",
        " [-1.  1.  1.]\n",
        " [-1.  1.  1.]\n",
        " [ 0. -1.  1.]\n",
        " [ 0. -1.  1.]\n",
        " [ 0. -1.  1.]\n",
        " [ 0. -1.  1.]\n",
        " [ 0. -1.  1.]\n",
        " [ 0. -1.  1.]\n",
        " [ 0. -1.  1.]\n",
        " [ 0. -1.  1.]\n",
        " [ 0. -1.  1.]\n",
        " [ 0. -1.  1.]]\n",
        "[[ 20. -10.   0.]\n",
        " [-10.  20.   0.]\n",
        " [  0.   0.  30.]]\n",
        "[[ 1.47543824]\n",
        " [ 0.08875154]]\n",
        "[[ 41.07694861]]\n",
        "[[ 41.07694861]]\n"
       ]
      }
     ],
     "prompt_number": 103
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "F test from contrast : dont look at this yet ...."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We assume from now on that the contrast of parameters we would like to test may\n",
      "concern multiple constraints on the $\\beta$, and will denote those constraints\n",
      "by $\\Lambda \\beta = 0$. \n",
      "\n",
      "Setting  $\\Lambda^T \\beta = 0$ is also setting  $H^T X \\beta =\n",
      "0$ for some matrix $H$ (the number of columns in $H$ is the number of columns\n",
      "in $\\Lambda$).  \n",
      "\n",
      "If we have a (legitimate) contrast of the parameters, what does\n",
      "$H$ look like? How does this relate to a reduced model? Why is all this\n",
      "relevant for fMRI?  It is interesting to understand what $H$ is. While $\\Lambda$\n",
      "puts some constraints on the parameters of the model, $H$ is the equivalent\n",
      "constraint on the space of $X$ \\citep{Christensen2002}. The reduced model that is to be tested is $ Y =\n",
      "X \\beta + \\epsilon $ $\\textbf{and}$ $H^T X \\beta =0$, or, equivalently, that $E(Y)\n",
      "\\in C(X)$ $\\textbf{and}$ $E(Y) \\in C(H)^{\\perp}$ where $C(H)^{\\perp}$ is the space\n",
      "orthogonal to $H$ (This is the set of vectors with zero correlation\n",
      "with any vector of $H$).  The reduced model should therefore be a matrix $X_0$\n",
      "such that $X_0 \\in C(X)$ $\\textbf{and}$ $X_0 \\in C(H)^{\\perp}$.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "By choosing $H = X^{T-} \\Lambda$, we impose the constraint on $X$ that\n",
      "corresponds to $\\Lambda^T \\beta = 0$.  Since $H^T E(Y) = H^T X \\beta =\n",
      "\\Lambda^T X^- X \\beta = \\Lambda^T \\beta = 0 $, we have $E(Y) \\in C(H)^\\perp $\n",
      "with this particular choice of $H$. Also, $H \\in C(X)$, so $H$ is a matrix \n",
      "that imposes a constraint on $X$ within the space of $X$, and is the part of\n",
      "$X$ that is being tested. The part of $X$ that is not being tested, the reduced\n",
      "model, can be found easily by projecting the orthogonal space of $H$ onto $X$ ,\n",
      "so we have $X_0 = P_X R_H  = P_X(I - P_H) = P_X - P_H$. Testing for $\\Lambda\n",
      "\\beta = 0$ is equivalent to testing for the reduced model $Y = X_0 \\gamma +\n",
      "\\epsilon$ with $X_0$ as defined above. Having defined $X_0$ as above, we have\n",
      "$P_X = P_{X_0} + P_H$.\n",
      "\n",
      "Now, the numerator of the F test can be rewritten in a much simpler\n",
      "way, as a function of $\\Lambda$ only:\n",
      "\n",
      "\\begin{eqnarray}\n",
      "  Y^T (P_H) Y & =  & Y^T X (X^T X)^- X^T H(H^T H)^- H^T X (X^T X)^- X^T Y / r(\\Lambda) \\\\ %\\nonumber\n",
      "  & =  & \\hat\\beta^T \\Lambda (\\Lambda^T (X^TX)^- \\Lambda)^- \\Lambda^T \\hat\\beta / r(\\Lambda)  \n",
      "\\end{eqnarray}\n",
      "Where $r(\\Lambda)$ is the rank of $\\Lambda$, and the F test can be rewritten as:\n",
      "\n",
      "\\begin{equation} \n",
      "  F_{\\nu_1, \\nu_2}  =  \\frac{\\hat\\beta^T \\Lambda (\\Lambda^T (X^TX)^- \\Lambda)^- \\Lambda^T \\hat\\beta / \\nu_{1} }{MSE} \\\\ \n",
      "\\end{equation}\n",
      "where the $MSE$ is the mean square error $SSR(X)/\\nu_2$.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 103
    }
   ],
   "metadata": {}
  }
 ]
}