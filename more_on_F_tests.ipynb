{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "The hidden truth on F-tests and contrasts "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We know that the traditional way of looking at an F-contrast is to think of two models : a full model $X$, and a reduced model $X_0$, and compute the sum of square of the residuals under these two models. We also know that we can express the numerator of an F test with a contrast that impose a constraint on the parameters of the model. This short notebook is to make clear the relation between these two formulations, and therefore being able to test unusual hypotheses more easily. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.linalg as lin\n",
      "import scipy.stats as sst\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 91
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n = 30\n",
      "sig = 1.\n",
      "Y = np.random.randn(n, sig)\n",
      "\n",
      "# make a design of 1 factor 3 levels\n",
      "oneway_anov = np.kron(np.eye(3), ones((10,1)))\n",
      "\n",
      "#add a trend?\n",
      "trend = np.arange(0,n).reshape(n,1)\n",
      "\n",
      "# add an intercept\n",
      "intercept = np.ones((n,1))\n",
      "\n",
      "X = np.hstack((oneway_anov, intercept))\n",
      "\n",
      "n, p = X.shape\n",
      "\n",
      "csignal = np.array([1., -1., 0, 3.]).reshape(4,1)\n",
      "signal = X.dot(csignal)\n",
      "Y +=  signal\n",
      "\n",
      "print np.hstack((Y, X))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 2.79360451  1.          0.          0.          1.        ]\n",
        " [ 3.20710956  1.          0.          0.          1.        ]\n",
        " [ 4.692603    1.          0.          0.          1.        ]\n",
        " [ 4.01924915  1.          0.          0.          1.        ]\n",
        " [ 4.4066797   1.          0.          0.          1.        ]\n",
        " [ 3.29325896  1.          0.          0.          1.        ]\n",
        " [ 5.38060672  1.          0.          0.          1.        ]\n",
        " [ 3.52488949  1.          0.          0.          1.        ]\n",
        " [ 4.45691233  1.          0.          0.          1.        ]\n",
        " [ 3.47854134  1.          0.          0.          1.        ]\n",
        " [ 2.32105402  0.          1.          0.          1.        ]\n",
        " [ 0.65703862  0.          1.          0.          1.        ]\n",
        " [ 2.99466321  0.          1.          0.          1.        ]\n",
        " [ 2.99825043  0.          1.          0.          1.        ]\n",
        " [ 0.97595565  0.          1.          0.          1.        ]\n",
        " [ 3.38428386  0.          1.          0.          1.        ]\n",
        " [ 2.53761108  0.          1.          0.          1.        ]\n",
        " [ 2.74427063  0.          1.          0.          1.        ]\n",
        " [ 1.85055038  0.          1.          0.          1.        ]\n",
        " [ 2.03403572  0.          1.          0.          1.        ]\n",
        " [ 3.58421366  0.          0.          1.          1.        ]\n",
        " [ 3.59337829  0.          0.          1.          1.        ]\n",
        " [ 1.99393093  0.          0.          1.          1.        ]\n",
        " [ 2.25893231  0.          0.          1.          1.        ]\n",
        " [ 3.87996898  0.          0.          1.          1.        ]\n",
        " [ 4.33233111  0.          0.          1.          1.        ]\n",
        " [ 3.42030964  0.          0.          1.          1.        ]\n",
        " [ 4.00180078  0.          0.          1.          1.        ]\n",
        " [ 4.06094657  0.          0.          1.          1.        ]\n",
        " [ 2.95960164  0.          0.          1.          1.        ]]\n"
       ]
      }
     ],
     "prompt_number": 92
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "F test with a known reduced model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's imaging that we know the reduced model: we'd like to test what is in $X$ but not in $X_0$.\n",
      "\n",
      "We define the orthogonal projector onto $X$ as $P_X = X X^-$, where $X^-$ is the moore-penrose pseudo inverse.\n",
      "\n",
      "\\begin{eqnarray} \n",
      "%\\label{equ:F} \n",
      "  F_{\\nu_1, \\nu_2} & = & \\frac{(Y^T(I-P_{X_0})Y - Y^T(I-P_{X})Y)/ \\nu_{1} }{Y^T(I-P_{X})Y/\\nu_{2}} \\\\ \n",
      "  \t\t& = & \\frac{(SSR(X_0) - SSR(X))/\\nu_1}{SSR(X)/\\nu_2}\n",
      "\\end{eqnarray}\n",
      "\n",
      "with $SSR(X)$, (resp. $SSR(X_0)$) the sum of square for error of model $X$ (resp. $X_0$), and\n",
      "\n",
      "\\begin{eqnarray} \n",
      "  \\nu_{1} & =  & tr(P_X - P_{X_0}) = tr(R_0 - R_X) \\\\ \\nonumber \\nu_{2} & = & tr(I - P_X) = tr(R_X) \n",
      "\\end{eqnarray}\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numpy.linalg import matrix_rank"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# lets do an F-test : \n",
      "\n",
      "def F_test(Y, X, L0):\n",
      "    '''\n",
      "    Y: numpy array\n",
      "        data\n",
      "    X: numpy array\n",
      "        design matrix\n",
      "    L0: numpy array\n",
      "        contrast forming X0, the \"null\" model\n",
      "    \n",
      "    returns: \n",
      "    \n",
      "    F, nu1, nu2 : float\n",
      "    '''\n",
      "    X0 = X.dot(L0)\n",
      "    n  = X.shape[0]\n",
      "    p0 = matrix_rank(X0)\n",
      "    p = matrix_rank(X)\n",
      "    \n",
      "    nu1 = p-p0\n",
      "    nu2 = n-p\n",
      "\n",
      "    SSR_num = (Y.T.dot(proj(X, Y)) - Y.T.dot(proj(X0, Y)))\n",
      "    numF = SSR_num/nu1\n",
      "    print '    ', SSR_num, p, p0\n",
      "    SSR_den = (Y.T.dot(R_proj(X, Y)))\n",
      "    denF = SSR_den/nu2\n",
      "    print '    ', SSR_den, n, p\n",
      "    \n",
      "    return numF/denF, nu1, nu2\n",
      "\n",
      "    \n",
      "def proj(X, Y=None):\n",
      "    # project onto the space of X\n",
      "    #print matrix_rank(X)\n",
      "    [u, s, vt] = lin.svd(X, full_matrices=False)\n",
      "    tol = s.max() * max(X.shape) * finfo(s.dtype).eps\n",
      "    nz = np.where(s > tol)[0]\n",
      "    #print nz\n",
      "    if Y is None:\n",
      "        return u[:,nz].dot(u[:,nz].T)\n",
      "    else:\n",
      "        return u[:,nz].dot(u[:,nz].T.dot(Y))\n",
      "    \n",
      "def R_proj(X, Y=None):\n",
      "    # project onto the space orthogonal to X (RY) \n",
      "    if Y is None:\n",
      "        return np.eye(X.shape[0]) - proj(X, Y) \n",
      "    else:\n",
      "        return Y - proj(X, Y)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 94
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Understanding the numerator : Matthew's proposal - works under some conditions:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At line $i$ :\n",
      "\n",
      "$$\n",
      "\\begin{eqnarray}\n",
      "\\epsilon_{0i}^2 - \\epsilon_i^2 &=& (Y - \\mu)^2  - (Y - \\beta_{i} - \\mu)^2 \\\\\n",
      "                            &=& \\epsilon_{0i}^2 - (\\epsilon_{0i} - \\beta_{i})^2 \\\\\n",
      "                            &=& \\beta_i^2 + 2\\epsilon_{0i}\\beta_{i}\n",
      "\\end{eqnarray}\n",
      "$$\n",
      "\n",
      "Summing these, we see that  $\\sum_i\\epsilon_{0i}\\beta_{i} = 0$ since each column of X must be decorrelated from the noise $\\epsilon_0$ "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A more general version of this can be written in the following way, with a small bit of matrix algebra:\n",
      "\n",
      "\n",
      "$$\n",
      "\\begin{eqnarray}\n",
      "SSR(X_0) - SSR(X) & = & (Y^T(I-P_{X_0})Y - Y^T(I-P_{X})Y)  \\\\ \n",
      "                & = & Y^TY - YP_{X_0}Y - Y^TY + YP_{X}Y \\\\\n",
      "                & = & Y^TP_{X}Y - Y^TP_{X_0}Y\n",
      "\\end{eqnarray}\n",
      "$$\n",
      "\n",
      "But $Y^TP_{X}Y$ is also $Y^TP_{X}P_{X}Y = Y^TX(X^TX)^{-1}X^T X(X^TX)^{-1}X^T Y =  \\beta^T X^T X \\beta$\n",
      "\n",
      "The first equality is because $P_{X} = P_{X}P_{X}$, the second by replacing  $P_X$ by $X(X^TX)^{-1}X^T $. \n",
      "\n",
      "Now, we can partition $X$ in $[X_t X_0]$ where $X_0$ is the null model and $X_t$ is the tested part (with parameters $\\beta_t$), and if they are orthogonal, then   \n",
      "\n",
      "$$\n",
      "SSR(X_0) - SSR(X)  = \\beta^T X^T X \\beta - \\beta_0^T X_0^T X_0 \\beta_0  =  \\beta_t^T X_t^T X_t \\beta_t\n",
      "$$\n",
      "\n",
      "and therefore\n",
      "\n",
      "$$\n",
      "SSR(X_0) - SSR(X)  = \\beta_t^T X_t^T X_t \\beta_t\n",
      "$$\n",
      "\n",
      "which corresponds to the sum of square of the means, weighted by the number of observation (see this by doing the dot product of $X_t \\beta_t$ by itself. Note what is $ X_t \\beta_t$ : the \"means\". "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "L0 = np.eye(p)[:,3:]\n",
      "(F, n1, n2) = F_test(Y, X, L0)\n",
      "print (F, n1, n2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "     [[ 14.72461023]] 3 1\n",
        "     [[ 18.3284104]] 30 3\n",
        "(array([[ 10.84557983]]), 2, 27)\n"
       ]
      }
     ],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# split X in Xt and X0 :\n",
      "X0 = X[:,[3]]\n",
      "# orthogonolize the three first columns of X : this is what we want to test\n",
      "Xt = R_proj(X0, X[:,:3])\n",
      "# recreate the all matrix\n",
      "Xall = np.hstack((Xt, X0))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 96
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "betah = lin.pinv(Xall).dot(Y)\n",
      "betah_t = betah[:3,:]\n",
      "\n",
      "print \"betah_t : \", betah_t\n",
      "\n",
      "tmp = Xt.dot(betah_t)\n",
      "SSRXt = tmp.T.dot(tmp)\n",
      "print \"Using the sum of square of the beta_t : SSRXt = %f \\n\" % SSRXt\n",
      "\n",
      "SSRX_SSRX0 =  Y.T.dot(proj(Xall, Y)) - Y.T.dot(proj(X0, Y))\n",
      "\n",
      "print \"Using SSR X - SSR X0 : SSRX_SSRX0 = %f \\n\" % SSRX_SSRX0\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "betah_t :  [[ 0.73079273]\n",
        " [-0.94478138]\n",
        " [ 0.21398865]]\n",
        "Using the sum of square of the beta_t : SSRXt = 14.724610 \n",
        "\n",
        "Using SSR X - SSR X0 : SSRX_SSRX0 = 14.724610 \n",
        "\n"
       ]
      }
     ],
     "prompt_number": 97
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.stats import f as Fdist\n",
      "Fdist.sf(F,n1,n2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 98,
       "text": [
        "array([[ 0.00034901]])"
       ]
      }
     ],
     "prompt_number": 98
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# check if we have no rank deficiency\n",
      "# recreate the original\n",
      "X = np.hstack((oneway_anov, intercept))\n",
      "\n",
      "Xt = X.dot(L)\n",
      "X0 = X[:,[3]]\n",
      "Xall = np.hstack((Xt, X0))\n",
      "\n",
      "betah = np.linalg.pinv(Xall).dot(Y)\n",
      "\n",
      "#print Xall\n",
      "#print Xall.T.dot(Xall)\n",
      "\n",
      "betah_t = betah[:2,:]\n",
      "print \" betah_t \", betah_t\n",
      "\n",
      "tmp = Xt.dot(betah_t)\n",
      "SSRXt = tmp.T.dot(tmp)\n",
      "print \"Using the sum of square of the beta_t : SSRXt = %f \\n\" % SSRXt\n",
      "\n",
      "SSRX_SSRX0 =  Y.T.dot(proj(Xall, Y)) - Y.T.dot(proj(X0, Y))\n",
      "\n",
      "print \"Using SSR X - SSR X0 : SSRX_SSRX0 = %f \\n\" % SSRX_SSRX0\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " betah_t  [[ 0.73079273]\n",
        " [-0.21398865]]\n",
        "Using the sum of square of the beta_t : SSRXt = 14.724610 \n",
        "\n",
        "Using SSR X - SSR X0 : SSRX_SSRX0 = 14.724610 \n",
        "\n"
       ]
      }
     ],
     "prompt_number": 99
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now - let's make the F test with a contrast. We just assume that we derived it - the formula is \n",
      "\n",
      "$$\n",
      "\\begin{equation} \n",
      "  F_{\\nu_1, \\nu_2}  =  \\frac{\\hat\\beta^T \\Lambda (\\Lambda^T (X^TX)^- \\Lambda)^- \\Lambda^T \\hat\\beta / \\nu_{1} }{MSE} \\\\ \n",
      "\\end{equation}\n",
      "$$\n",
      "\n",
      "where the $MSE$ is the mean square error $SSR(X)/\\nu_2$.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def F_con(X, Y, L):\n",
      "    \n",
      "    # check that the contrast makes sense\n",
      "    B = lin.pinv(X).dot(Y)\n",
      "    \n",
      "    # Numerator\n",
      "    piXtX = lin.pinv(X.T.dot(X))\n",
      "    piLtpiXtXL = lin.pinv(L.T.dot(piXtX).dot(L))\n",
      "    LtB = L.T.dot(B)\n",
      "    Fnum = LtB.T.dot(piLtpiXtXL).dot(LtB)\n",
      "    \n",
      "    # SSRX ?\n",
      "    SSRX = Y.T.dot(Y) - B.T.dot(X.T.dot(X)).dot(B)\n",
      "    print Fnum, SSRX\n",
      "    \n",
      "    v1 = matrix_rank(L)\n",
      "    v2 = X.shape[0] - matrix_rank(X)\n",
      "    \n",
      "    return (Fnum/v1)/(SSRX/v2), v1, v2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 105
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "B = lin.pinv(X).dot(Y)\n",
      "print B\n",
      "#print X\n",
      "L = np.array([[1., -1., 0, 0],[0., 1., -1., 0]]).T\n",
      "print L\n",
      "F,v1,v2 = F_con(X, Y, L)\n",
      "print F,v1,v2 "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 1.52943092]\n",
        " [-0.1461432 ]\n",
        " [ 1.01262684]\n",
        " [ 2.39591456]]\n",
        "[[ 1.  0.]\n",
        " [-1.  1.]\n",
        " [ 0. -1.]\n",
        " [ 0.  0.]]\n",
        "[[ 14.72461023]] [[ 18.3284104]]\n",
        "[[ 10.84557983]] 2 27\n"
       ]
      }
     ],
     "prompt_number": 106
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "F test from contrast : the derivation - you dont need to look at this  ...."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We assume from now on that the contrast of parameters we would like to test may\n",
      "concern multiple constraints on the $\\beta$, and will denote those constraints\n",
      "by $\\Lambda \\beta = 0$. \n",
      "\n",
      "Setting  $\\Lambda^T \\beta = 0$ is also setting  $H^T X \\beta =\n",
      "0$ for some matrix $H$ (the number of columns in $H$ is the number of columns\n",
      "in $\\Lambda$).  \n",
      "\n",
      "If we have a (legitimate) contrast of the parameters, what does\n",
      "$H$ look like? How does this relate to a reduced model? Why is all this\n",
      "relevant for fMRI?  It is interesting to understand what $H$ is. While $\\Lambda$\n",
      "puts some constraints on the parameters of the model, $H$ is the equivalent\n",
      "constraint on the space of $X$ \\citep{Christensen2002}. The reduced model that is to be tested is $ Y =\n",
      "X \\beta + \\epsilon $ $\\textbf{and}$ $H^T X \\beta =0$, or, equivalently, that $E(Y)\n",
      "\\in C(X)$ $\\textbf{and}$ $E(Y) \\in C(H)^{\\perp}$ where $C(H)^{\\perp}$ is the space\n",
      "orthogonal to $H$ (This is the set of vectors with zero correlation\n",
      "with any vector of $H$).  The reduced model should therefore be a matrix $X_0$\n",
      "such that $X_0 \\in C(X)$ $\\textbf{and}$ $X_0 \\in C(H)^{\\perp}$.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "By choosing $H = X^{T-} \\Lambda$, we impose the constraint on $X$ that\n",
      "corresponds to $\\Lambda^T \\beta = 0$.  Since $H^T E(Y) = H^T X \\beta =\n",
      "\\Lambda^T X^- X \\beta = \\Lambda^T \\beta = 0 $, we have $E(Y) \\in C(H)^\\perp $\n",
      "with this particular choice of $H$. Also, $H \\in C(X)$, so $H$ is a matrix \n",
      "that imposes a constraint on $X$ within the space of $X$, and is the part of\n",
      "$X$ that is being tested. The part of $X$ that is not being tested, the reduced\n",
      "model, can be found easily by projecting the orthogonal space of $H$ onto $X$ ,\n",
      "so we have $X_0 = P_X R_H  = P_X(I - P_H) = P_X - P_H$. Testing for $\\Lambda\n",
      "\\beta = 0$ is equivalent to testing for the reduced model $Y = X_0 \\gamma +\n",
      "\\epsilon$ with $X_0$ as defined above. Having defined $X_0$ as above, we have\n",
      "$P_X = P_{X_0} + P_H$.\n",
      "\n",
      "Now, the numerator of the F test can be rewritten in a much simpler\n",
      "way, as a function of $\\Lambda$ only:\n",
      "\n",
      "\\begin{eqnarray}\n",
      "  Y^T (P_H) Y & =  & Y^T X (X^T X)^- X^T H(H^T H)^- H^T X (X^T X)^- X^T Y / r(\\Lambda) \\\\ %\\nonumber\n",
      "  & =  & \\hat\\beta^T \\Lambda (\\Lambda^T (X^TX)^- \\Lambda)^- \\Lambda^T \\hat\\beta / r(\\Lambda)  \n",
      "\\end{eqnarray}\n",
      "Where $r(\\Lambda)$ is the rank of $\\Lambda$, and the F test can be rewritten as:\n",
      "\n",
      "\\begin{equation} \n",
      "  F_{\\nu_1, \\nu_2}  =  \\frac{\\hat\\beta^T \\Lambda (\\Lambda^T (X^TX)^- \\Lambda)^- \\Lambda^T \\hat\\beta / \\nu_{1} }{MSE} \\\\ \n",
      "\\end{equation}\n",
      "where the $MSE$ is the mean square error $SSR(X)/\\nu_2$.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 101
    }
   ],
   "metadata": {}
  }
 ]
}